{
    "trial": 17,
    "fold": 1,
    "epoch": 1,
    "val_acc": 0.1590909090909091,
    "val_f1": 0.07874196510560147,
    "hyperparameters": {
        "d_model": 256,
        "num_heads": 8,
        "num_layers": 3,
        "dropout": 0.2234072371415879,
        "embedding_dim": 16,
        "learning_rate": 0.0016297937932624438,
        "batch_size": 32
    },
    "num_parameters": 1217706,
    "model_architecture": "ActivityTransformer(\n  (subject_embedding): Embedding(10, 16)\n  (input_proj): Linear(in_features=55, out_features=256, bias=True)\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.2234072371415879, inplace=False)\n        (linear2): Linear(in_features=256, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.2234072371415879, inplace=False)\n        (dropout2): Dropout(p=0.2234072371415879, inplace=False)\n      )\n    )\n  )\n  (classifier): Linear(in_features=256, out_features=10, bias=True)\n)"
}